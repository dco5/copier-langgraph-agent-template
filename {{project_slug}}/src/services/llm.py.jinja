"""
LLM service with automatic retry logic, observability, and multi-provider support.

Supports runtime switching between providers: OpenAI, Anthropic, Azure OpenAI, and Ollama.
"""

import time
from collections.abc import AsyncGenerator
from typing import Any, Literal

from langchain_core.language_models import BaseChatModel
from langchain_core.messages import BaseMessage
{%- if observability == 'langfuse' %}
from langfuse.callback import CallbackHandler as LangfuseCallbackHandler
{%- elif observability == 'langsmith' %}
from langsmith import Client as LangSmithClient
{%- endif %}
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_exponential,
    before_sleep_log,
)

from src.core.config import settings
from src.core.logging import get_logger
{%- if use_prometheus %}
from src.core.metrics import record_llm_request, llm_retry_count
{%- endif %}

logger = get_logger("llm")

# Type alias for supported providers
LLMProvider = Literal["openai", "anthropic", "azure", "ollama"]

# Exceptions that should trigger retry
RETRYABLE_EXCEPTIONS = (
    TimeoutError,
    ConnectionError,
)


def _create_openai_llm(
    model: str,
    temperature: float,
    max_tokens: int,
    timeout: int,
    **kwargs: Any,
) -> BaseChatModel:
    """Create OpenAI LLM instance."""
    try:
        from langchain_openai import ChatOpenAI
    except ImportError:
        raise ImportError(
            "langchain-openai is not installed. "
            "Install it with: pip install langchain-openai "
            "or: pip install '{{ project_slug }}[openai]'"
        )

    if not settings.openai_api_key:
        raise ValueError("OpenAI API key not configured. Set OPENAI_API_KEY environment variable.")

    return ChatOpenAI(
        model=model,
        api_key=settings.openai_api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
        **kwargs,
    )


def _create_anthropic_llm(
    model: str,
    temperature: float,
    max_tokens: int,
    timeout: int,
    **kwargs: Any,
) -> BaseChatModel:
    """Create Anthropic LLM instance."""
    try:
        from langchain_anthropic import ChatAnthropic
    except ImportError:
        raise ImportError(
            "langchain-anthropic is not installed. "
            "Install it with: pip install langchain-anthropic "
            "or: pip install '{{ project_slug }}[anthropic]'"
        )

    if not settings.anthropic_api_key:
        raise ValueError(
            "Anthropic API key not configured. Set ANTHROPIC_API_KEY environment variable."
        )

    return ChatAnthropic(
        model=model,
        api_key=settings.anthropic_api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
        **kwargs,
    )


def _create_azure_llm(
    model: str,
    temperature: float,
    max_tokens: int,
    timeout: int,
    **kwargs: Any,
) -> BaseChatModel:
    """Create Azure OpenAI LLM instance."""
    try:
        from langchain_openai import AzureChatOpenAI
    except ImportError:
        raise ImportError(
            "langchain-openai is not installed. "
            "Install it with: pip install langchain-openai "
            "or: pip install '{{ project_slug }}[openai]'"
        )

    if not settings.azure_openai_api_key or not settings.azure_openai_endpoint:
        raise ValueError(
            "Azure OpenAI not configured. Set AZURE_OPENAI_API_KEY and "
            "AZURE_OPENAI_ENDPOINT environment variables."
        )

    return AzureChatOpenAI(
        azure_deployment=model,
        api_key=settings.azure_openai_api_key,
        azure_endpoint=settings.azure_openai_endpoint,
        api_version=settings.azure_openai_api_version,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
        **kwargs,
    )


def _create_ollama_llm(
    model: str,
    temperature: float,
    max_tokens: int,
    timeout: int,
    **kwargs: Any,
) -> BaseChatModel:
    """Create Ollama LLM instance."""
    try:
        from langchain_ollama import ChatOllama
    except ImportError:
        raise ImportError(
            "langchain-ollama is not installed. "
            "Install it with: pip install langchain-ollama "
            "or: pip install '{{ project_slug }}[ollama]'"
        )

    return ChatOllama(
        model=model,
        base_url=settings.ollama_base_url,
        temperature=temperature,
        num_predict=max_tokens,
        timeout=timeout,
        **kwargs,
    )


# Provider factory mapping
_LLM_FACTORIES = {
    "openai": _create_openai_llm,
    "anthropic": _create_anthropic_llm,
    "azure": _create_azure_llm,
    "ollama": _create_ollama_llm,
}


class LLMService:
    """
    LLM service with:
    - Multi-provider support (OpenAI, Anthropic, Azure, Ollama)
    - Runtime provider switching via LLM_PROVIDER env var
    - Automatic retry with exponential backoff
{%- if observability != 'none' %}
    - {{ observability | title }} observability integration
{%- endif %}
    - Token counting and metrics
    """

    def __init__(
        self,
        model: str | None = None,
        provider: LLMProvider | None = None,
        temperature: float | None = None,
        max_tokens: int | None = None,
        **kwargs: Any,
    ):
        self.provider = provider or settings.llm_provider
        self.model = model or settings.default_llm_model
        self.temperature = temperature if temperature is not None else settings.default_llm_temperature
        self.max_tokens = max_tokens or settings.max_tokens
        self.extra_kwargs = kwargs

        self._llm: BaseChatModel | None = None
{%- if observability == 'langfuse' %}
        self._langfuse_handler: LangfuseCallbackHandler | None = None
{%- endif %}

    @property
    def llm(self) -> BaseChatModel:
        """Get or create the LLM instance."""
        if self._llm is None:
            self._llm = self._create_llm()
        return self._llm

{%- if observability == 'langfuse' %}

    @property
    def langfuse_handler(self) -> LangfuseCallbackHandler | None:
        """Get Langfuse callback handler for observability."""
        if self._langfuse_handler is None and settings.langfuse_enabled:
            if settings.langfuse_public_key and settings.langfuse_secret_key:
                self._langfuse_handler = LangfuseCallbackHandler(
                    public_key=settings.langfuse_public_key,
                    secret_key=settings.langfuse_secret_key,
                    host=settings.langfuse_host,
                )
        return self._langfuse_handler
{%- endif %}

    def _create_llm(self) -> BaseChatModel:
        """Create LLM instance based on configured provider."""
        factory = _LLM_FACTORIES.get(self.provider)
        if not factory:
            raise ValueError(
                f"Unknown LLM provider: {self.provider}. "
                f"Supported providers: {list(_LLM_FACTORIES.keys())}"
            )

        logger.debug(
            "creating_llm",
            provider=self.provider,
            model=self.model,
        )

        return factory(
            model=self.model,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            timeout=settings.llm_request_timeout,
            **self.extra_kwargs,
        )

    def get_callbacks(self, **extra_metadata: Any) -> list:
        """Get callbacks for LLM invocation."""
        callbacks = []

{%- if observability == 'langfuse' %}
        if self.langfuse_handler:
            callbacks.append(self.langfuse_handler)
{%- endif %}

        return callbacks

    @retry(
        retry=retry_if_exception_type(RETRYABLE_EXCEPTIONS),
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        before_sleep=before_sleep_log(logger, "WARNING"),
    )
    async def invoke(
        self,
        messages: list[BaseMessage],
        **kwargs: Any,
    ) -> BaseMessage:
        """
        Invoke LLM with automatic retry on transient failures.
        """
        start_time = time.perf_counter()
        status = "success"

        try:
            callbacks = self.get_callbacks(**kwargs.pop("metadata", {}))

            response = await self.llm.ainvoke(
                messages,
                config={"callbacks": callbacks},
                **kwargs,
            )

            return response

        except RETRYABLE_EXCEPTIONS as e:
            status = "retry"
{%- if use_prometheus %}
            llm_retry_count.labels(model=self.model, reason=type(e).__name__).inc()
{%- endif %}
            raise

        except Exception as e:
            status = "error"
            logger.error(
                "llm_invocation_failed",
                provider=self.provider,
                model=self.model,
                error=str(e),
            )
            raise

        finally:
            duration = time.perf_counter() - start_time
{%- if use_prometheus %}
            record_llm_request(self.model, status, duration)
{%- endif %}

            logger.debug(
                "llm_invocation_completed",
                provider=self.provider,
                model=self.model,
                duration=round(duration, 3),
                status=status,
            )

    async def stream(
        self,
        messages: list[BaseMessage],
        **kwargs: Any,
    ) -> AsyncGenerator[str, None]:
        """
        Stream LLM response tokens.
        """
        start_time = time.perf_counter()

        try:
            callbacks = self.get_callbacks(**kwargs.pop("metadata", {}))

            async for chunk in self.llm.astream(
                messages,
                config={"callbacks": callbacks},
                **kwargs,
            ):
                if chunk.content:
                    yield chunk.content

            duration = time.perf_counter() - start_time
{%- if use_prometheus %}
            record_llm_request(self.model, "success", duration)
{%- endif %}

        except Exception as e:
            duration = time.perf_counter() - start_time
{%- if use_prometheus %}
            record_llm_request(self.model, "error", duration)
{%- endif %}
            logger.error(
                "llm_stream_failed",
                provider=self.provider,
                model=self.model,
                error=str(e),
            )
            raise


def get_llm(
    model: str | None = None,
    provider: LLMProvider | None = None,
    temperature: float | None = None,
    **kwargs: Any,
) -> BaseChatModel:
    """
    Get a configured LLM instance.

    Args:
        model: Model name (e.g., 'gpt-4o', 'claude-sonnet-4-20250514')
        provider: LLM provider ('openai', 'anthropic', 'azure', 'ollama').
                  Defaults to LLM_PROVIDER env var.
        temperature: Sampling temperature
        **kwargs: Additional provider-specific arguments

    Returns:
        Configured LLM instance
    """
    service = LLMService(
        model=model,
        provider=provider,
        temperature=temperature,
        **kwargs,
    )
    return service.llm


def get_llm_service(
    model: str | None = None,
    provider: LLMProvider | None = None,
    temperature: float | None = None,
    **kwargs: Any,
) -> LLMService:
    """
    Get an LLM service instance with full functionality.

    Args:
        model: Model name
        provider: LLM provider. Defaults to LLM_PROVIDER env var.
        temperature: Sampling temperature
        **kwargs: Additional provider-specific arguments

    Returns:
        LLMService instance with invoke/stream methods
    """
    return LLMService(
        model=model,
        provider=provider,
        temperature=temperature,
        **kwargs,
    )
